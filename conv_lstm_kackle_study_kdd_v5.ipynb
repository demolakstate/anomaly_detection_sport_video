{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv_lstm_kackle_study_kdd_v5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMfU9qq4aLoBP4eKhmMwJUA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/demolakstate/anomaly_detection_sport_video/blob/main/conv_lstm_kackle_study_kdd_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s1f2_Qgo4m9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  \n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"conv_lstm_kackle_study_v2\n",
        "Automatically generated by Colaboratory.\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1vw2RPDDU-02Hst2Ef4N0s_zkqGdBX98q\n",
        "# Next-Frame Video Prediction with Convolutional LSTMs\n",
        "**Author:** [Amogh Joshi](https://github.com/amogh7joshi)<br>\n",
        "**Date created:** 2021/06/02<br>\n",
        "**Last modified:** 2021/06/05<br>\n",
        "**Description:** How to build and train a convolutional LSTM model for next-frame video prediction.\n",
        "## Introduction\n",
        "The\n",
        "[Convolutional LSTM](https://papers.nips.cc/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf)\n",
        "architectures bring together time series processing and computer vision by\n",
        "introducing a convolutional recurrent cell in a LSTM layer. In this example, we will explore the\n",
        "Convolutional LSTM model in an application to next-frame prediction, the process\n",
        "of predicting what video frames come next given a series of past frames.\n",
        "## Setup\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import io\n",
        "import imageio\n",
        "from IPython.display import Image, display\n",
        "from ipywidgets import widgets, Layout, HBox\n",
        "import cv2\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "iAwIMOn64oE_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"##Define hyperparameters\"\"\"\n",
        "\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 768#3072#768#1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"## Dataset Construction\n",
        "For this example, we will be using the\n",
        "[Moving MNIST](http://www.cs.toronto.edu/~nitish/unsupervised_video/)\n",
        "dataset.\n",
        "We will download the dataset and then construct and\n",
        "preprocess training and validation sets.\n",
        "For next-frame prediction, our model will be using a previous frame,\n",
        "which we'll call `f_n`, to predict a new frame, called `f_(n + 1)`.\n",
        "To allow the model to create these predictions, we'll need to process\n",
        "the data such that we have \"shifted\" inputs and outputs, where the\n",
        "input data is frame `x_n`, being used to predict frame `y_(n + 1)`.\n",
        "##Data collection\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "VKy2z0ZH4oJZ",
        "outputId": "3718ea2a-e5a8-42b0-eee0-a2956957f08a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'## Dataset Construction\\nFor this example, we will be using the\\n[Moving MNIST](http://www.cs.toronto.edu/~nitish/unsupervised_video/)\\ndataset.\\nWe will download the dataset and then construct and\\npreprocess training and validation sets.\\nFor next-frame prediction, our model will be using a previous frame,\\nwhich we\\'ll call `f_n`, to predict a new frame, called `f_(n + 1)`.\\nTo allow the model to create these predictions, we\\'ll need to process\\nthe data such that we have \"shifted\" inputs and outputs, where the\\ninput data is frame `x_n`, being used to predict frame `y_(n + 1)`.\\n##Data collection\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MliP1Zu4odn",
        "outputId": "353a2865-0a96-4c97-9182-23955d649eeb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            #frame = crop_center(frame)\n",
        "            # resize frames\n",
        "            frame = cv2.resize(frame, (128,128))\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            #print(f'frame shape: {frame.shape}')\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "#frames = load_video('/content/gdrive/MyDrive/ksutackle_dataset/risky_7.mp4', 20)\n",
        "\n",
        "#frames.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "tE6RVbfM41sD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let extract features\n",
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.DenseNet121(weights=\"imagenet\",include_top=False,pooling=\"avg\",\n",
        "                        input_shape=(IMG_SIZE, IMG_SIZE, 3), )\n",
        "    preprocess_input = keras.applications.densenet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    # Add a dense layer\n",
        "    outputs = layers.Dense(768)(outputs)\n",
        "\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ],
      "metadata": {
        "id": "QEouphj76L2d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for layer in feature_extractor.layers[:-1]:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "9F9cjb-8fQdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxJbdcwDTsbK",
        "outputId": "6095bc16-e5da-4a86-d01a-65ff3b4f4c00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"feature_extractor\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " tf.math.truediv (TFOpLambda  (None, 128, 128, 3)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " tf.nn.bias_add (TFOpLambda)  (None, 128, 128, 3)      0         \n",
            "                                                                 \n",
            " tf.math.truediv_1 (TFOpLamb  (None, 128, 128, 3)      0         \n",
            " da)                                                             \n",
            "                                                                 \n",
            " densenet121 (Functional)    (None, 1024)              7037504   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 768)               787200    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,824,704\n",
            "Trainable params: 7,741,056\n",
            "Non-trainable params: 83,648\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_all_videos(root_dir, frames_all=[]):\n",
        "    \n",
        "    video_paths = os.listdir(root_dir)\n",
        "    labels = [video_path.split('_')[0] for video_path in video_paths]\n",
        "    num_samples = len(labels)\n",
        "    \n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Call visualization here\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[:20]\n",
        "        frames = frames[None, ...]\n",
        "         # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :] )\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    print(frame_features.shape)\n",
        "\n",
        "    return frame_features\n",
        "\n",
        "        #frames_all.append(frames)\n",
        "\n",
        "    #return np.array(frames_all)\n"
      ],
      "metadata": {
        "id": "vgkwgo-m5dls"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cap = cv2.VideoCapture('/content/gdrive/MyDrive/ksutackle_dataset/risky_7.mp4')\n",
        "ret, frame = cap.read()\n",
        "i = 0\n",
        "\n",
        "while ret:\n",
        "  # try:\n",
        "  #   ret, frame = cap.read()\n",
        "  #   if not ret:\n",
        "  #     break\n",
        "    #cv2_imshow(frame)\n",
        "    #frame = crop_center(frame)\n",
        "    #cv2.imwrite(f'frame_{i}.jpg', frame)\n",
        "    ret, frame = cap.read()\n",
        "    i += 1\n",
        "    #cv2_imshow(frame)\n",
        "    #frame = frame[:,:,[1,2,0]]\n",
        "    #print(frame.shape)\n",
        "    #cv2_imshow(frame)\n",
        "\n",
        "\n",
        "  # finally:\n",
        "  #   cap.release()"
      ],
      "metadata": {
        "id": "TWGdExen5dps"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Kgxt2bl25dtu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axRgKahF4O8s",
        "outputId": "aa446fb1-af46-43bf-9800-02b4dc349935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data from disk\n",
            "Frame features in train set: (75, 20, 768)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    data = np.load(\"data_conv_lstm.npy\")\n",
        "    print(\"Successfully loaded data from disk\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Dataset not available on disk, preparing a new one...\")\n",
        "    #data = prepare_all_videos('../../ksutackle_dataset_108')\n",
        "    data = prepare_all_videos('/content/gdrive/MyDrive/ksutackle_dataset')\n",
        "    np.save(\"data_conv_lstm.npy\", data)\n",
        "    #np.save(\"labels.npy\", labels)\n",
        "\n",
        "train_data_all, test_data  = train_test_split(data, test_size=0.30, random_state=42)\n",
        "#train_data, val_data, train_labels, val_labels = train_test_split(train_data_all, train_labels_all, test_size=0.20, random_state=45)\n",
        "print(f\"Frame features in train set: {train_data_all.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Normalize the data to the 0-1 range.\n",
        "train_dataset = train_data_all / 255\n",
        "val_dataset = test_data / 255\n",
        "\n",
        "# We'll define a helper function to shift the frames, where\n",
        "# `x` is frames 0 to n - 1, and `y` is frames 1 to n.\n",
        "def create_shifted_frames(data):\n",
        "    # x = data[:, 0 : data.shape[1] - 1, :, :]\n",
        "    # y = data[:, 1 : data.shape[1], :, :]\n",
        "    x = data[:, 0 : data.shape[1] - 1, :]\n",
        "    y = data[:, 1 : data.shape[1], :]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# Apply the processing function to the datasets.\n",
        "x_train, y_train = create_shifted_frames(train_dataset)\n",
        "x_val, y_val = create_shifted_frames(val_dataset)\n",
        "\n",
        "# Inspect the dataset.\n",
        "print(\"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape))\n",
        "print(\"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ehxgPZItx4",
        "outputId": "e97182f2-76d9-4344-ced3-736f233328c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Shapes: (75, 19, 768), (75, 19, 768)\n",
            "Validation Dataset Shapes: (33, 19, 768), (33, 19, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = layers.Reshape((-1,16,16,3))(x_train)\n",
        "y_train = layers.Reshape((-1,16,16,3))(y_train)\n",
        "\n",
        "x_val = layers.Reshape((-1,16,16,3))(x_val)\n",
        "y_val = layers.Reshape((-1,16,16,3))(y_val)"
      ],
      "metadata": {
        "id": "Fm76UhHXKOH6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VEFoXDe-KONi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"## Data Visualization\n",
        "Our data consists of sequences of frames, each of which\n",
        "are used to predict the upcoming frame. Let's take a look\n",
        "at some of these sequential frames.\n",
        "\"\"\"\n",
        "\n",
        "# Construct a figure on which we will visualize the images.\n",
        "fig, axes = plt.subplots(4, 5, figsize=(10, 8))\n",
        "\n",
        "# Plot each of the sequential images for one random data example.\n",
        "data_choice = np.random.choice(range(len(train_dataset)), size=1)[0]\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "    #ax.imshow(np.squeeze(train_dataset[data_choice][idx]), cmap=\"gray\")\n",
        "    ax.imshow(y_train[data_choice][idx], cmap=\"brg\")\n",
        "    ax.set_title(f\"Frame {idx + 1}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Print information and display the figure.\n",
        "print(f\"Displaying frames for example {data_choice}.\")\n",
        "plt.show()\n",
        "\n",
        "\"\"\"## Model Construction\n",
        "To build a Convolutional LSTM model, we will use the\n",
        "`ConvLSTM2D` layer, which will accept inputs of shape\n",
        "`(batch_size, num_frames, width, height, channels)`, and return\n",
        "a prediction movie of the same shape.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cbaaYsZwIt1t",
        "outputId": "9695770f-3654-44a8-b725-9b08e7e64d24"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fbaf1b8e5f79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#ax.imshow(np.squeeze(train_dataset[data_choice][idx]), cmap=\"gray\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_choice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"brg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Frame {idx + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: slice index 19 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 20 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SpxRdKnOJq6O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip('horizontal'),tf.keras.layers.RandomRotation(0.2),])\n",
        "# preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "# # Create the base model from the pre-trained model MobileNet V2\n",
        "# IMG_SHAPE = IMG_SIZE + (3,)\n",
        "# base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "# #image_batch, label_batch = next(iter(train_dataset))\n",
        "# feature_batch = base_model(x_train[0])\n",
        "# print(feature_batch.shape)"
      ],
      "metadata": {
        "id": "8KkfRlIOIt5n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OcjsHzd8It9I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Construct the input layer with no definite frame size.\n",
        "inp = layers.Input(shape=(None, *x_train.shape[2:]))\n",
        "\n",
        "#x = layers.Reshape((-1,16,16,3))(inp)\n",
        "\n",
        "print(inp.shape)\n",
        "\n",
        "# We will construct 3 `ConvLSTM2D` layers with batch normalization,\n",
        "# followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=64,\n",
        "    kernel_size=(5, 5),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(inp)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=64,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=64,\n",
        "    kernel_size=(1, 1),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.Conv3D(\n",
        "    filters=3, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
        ")(x)\n",
        "\n",
        "print(inp.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuToSoel6cH0",
        "outputId": "874b111f-b51e-4726-8c4a-126d90515d0f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 16, 16, 3)\n",
            "(None, None, 16, 16, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we will build the complete model and compile it.\n",
        "model = keras.models.Model(inp, x)\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),\n",
        ")\n",
        "\n",
        "\"\"\"## Model Training\n",
        "With our model and data constructed, we can now train the model.\n",
        "\"\"\"\n",
        "\n",
        "# Define some callbacks to improve training.\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "# Define modifiable training hyperparameters.\n",
        "epochs = 20\n",
        "batch_size = 5\n",
        "\n",
        "# Fit the model to the training data.\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        ")\n",
        "\n",
        "\n",
        "model.save('model_conv_lstm_v2.h5')\n",
        "\"\"\"## Frame Prediction Visualizations\n",
        "With our model now constructed and trained, we can generate\n",
        "some example frame predictions based on a new video.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "D6HGoM0MLQp3",
        "outputId": "98a99506-976e-4be6-e0ef-1402244cde8e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 73s 5s/step - loss: -2198.7778 - val_loss: -2.8983 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 66s 4s/step - loss: -27531.9609 - val_loss: -96.2513 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 76s 5s/step - loss: -1794975.7500 - val_loss: -5314.1753 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 81s 5s/step - loss: -256380608.0000 - val_loss: -5244810.0000 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 84s 6s/step - loss: -4294412544.0000 - val_loss: -50241528.0000 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 79s 5s/step - loss: -167070744576.0000 - val_loss: -10365465600.0000 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 76s 5s/step - loss: -6421782462464.0000 - val_loss: -893254434816.0000 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 66s 4s/step - loss: -604911381250048.0000 - val_loss: -3826501550080.0000 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 67s 4s/step - loss: -41694219659640832.0000 - val_loss: -7486230539272192.0000 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 66s 4s/step - loss: -238651043213213696.0000 - val_loss: -42547758510374912.0000 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 71s 5s/step - loss: -939390917654085632.0000 - val_loss: -200632645544050688.0000 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 66s 4s/step - loss: -3867074478987018240.0000 - val_loss: -19570974114447360.0000 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 66s 4s/step - loss: -233175784544534528.0000 - val_loss: -83453877441003520.0000 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 66s 4s/step - loss: nan - val_loss: nan - lr: 0.0010\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 66s 4s/step - loss: nan - val_loss: nan - lr: 0.0010\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 66s 4s/step - loss: nan - val_loss: nan - lr: 0.0010\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 66s 4s/step - loss: nan - val_loss: nan - lr: 1.0000e-04\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 69s 5s/step - loss: nan - val_loss: nan - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 66s 4s/step - loss: nan - val_loss: nan - lr: 1.0000e-04\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 66s 4s/step - loss: nan - val_loss: nan - lr: 1.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'## Frame Prediction Visualizations\\nWith our model now constructed and trained, we can generate\\nsome example frame predictions based on a new video.'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZWhPQjX66cMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "laycUNzQ6cQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}