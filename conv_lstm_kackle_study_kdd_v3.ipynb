{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv_lstm_kackle_study_kdd_v3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyVsFTYhj4cA7fZEwYHH4V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/demolakstate/anomaly_detection_sport_video/blob/main/conv_lstm_kackle_study_kdd_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s1f2_Qgo4m9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  \n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"conv_lstm_kackle_study_v2\n",
        "Automatically generated by Colaboratory.\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1vw2RPDDU-02Hst2Ef4N0s_zkqGdBX98q\n",
        "# Next-Frame Video Prediction with Convolutional LSTMs\n",
        "**Author:** [Amogh Joshi](https://github.com/amogh7joshi)<br>\n",
        "**Date created:** 2021/06/02<br>\n",
        "**Last modified:** 2021/06/05<br>\n",
        "**Description:** How to build and train a convolutional LSTM model for next-frame video prediction.\n",
        "## Introduction\n",
        "The\n",
        "[Convolutional LSTM](https://papers.nips.cc/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf)\n",
        "architectures bring together time series processing and computer vision by\n",
        "introducing a convolutional recurrent cell in a LSTM layer. In this example, we will explore the\n",
        "Convolutional LSTM model in an application to next-frame prediction, the process\n",
        "of predicting what video frames come next given a series of past frames.\n",
        "## Setup\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import io\n",
        "import imageio\n",
        "from IPython.display import Image, display\n",
        "from ipywidgets import widgets, Layout, HBox\n",
        "import cv2\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "iAwIMOn64oE_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"##Define hyperparameters\"\"\"\n",
        "\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 1024#3072#768#1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"## Dataset Construction\n",
        "For this example, we will be using the\n",
        "[Moving MNIST](http://www.cs.toronto.edu/~nitish/unsupervised_video/)\n",
        "dataset.\n",
        "We will download the dataset and then construct and\n",
        "preprocess training and validation sets.\n",
        "For next-frame prediction, our model will be using a previous frame,\n",
        "which we'll call `f_n`, to predict a new frame, called `f_(n + 1)`.\n",
        "To allow the model to create these predictions, we'll need to process\n",
        "the data such that we have \"shifted\" inputs and outputs, where the\n",
        "input data is frame `x_n`, being used to predict frame `y_(n + 1)`.\n",
        "##Data collection\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "VKy2z0ZH4oJZ",
        "outputId": "6df7b785-c223-4f5e-b302-183a85afc2ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'## Dataset Construction\\nFor this example, we will be using the\\n[Moving MNIST](http://www.cs.toronto.edu/~nitish/unsupervised_video/)\\ndataset.\\nWe will download the dataset and then construct and\\npreprocess training and validation sets.\\nFor next-frame prediction, our model will be using a previous frame,\\nwhich we\\'ll call `f_n`, to predict a new frame, called `f_(n + 1)`.\\nTo allow the model to create these predictions, we\\'ll need to process\\nthe data such that we have \"shifted\" inputs and outputs, where the\\ninput data is frame `x_n`, being used to predict frame `y_(n + 1)`.\\n##Data collection\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MliP1Zu4odn",
        "outputId": "d198b492-13b7-45d9-8787-7c2a79773bad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            #frame = crop_center(frame)\n",
        "            # resize frames\n",
        "            frame = cv2.resize(frame, (128,128))\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            #print(f'frame shape: {frame.shape}')\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "#frames = load_video('/content/gdrive/MyDrive/ksutackle_dataset/risky_7.mp4', 20)\n",
        "\n",
        "#frames.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "tE6RVbfM41sD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let extract features\n",
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.DenseNet121(weights=\"imagenet\",include_top=False,pooling=\"avg\",\n",
        "                        input_shape=(IMG_SIZE, IMG_SIZE, 3), )\n",
        "    preprocess_input = keras.applications.densenet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ],
      "metadata": {
        "id": "QEouphj76L2d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_all_videos(root_dir, frames_all=[]):\n",
        "    \n",
        "    video_paths = os.listdir(root_dir)\n",
        "    labels = [video_path.split('_')[0] for video_path in video_paths]\n",
        "    num_samples = len(labels)\n",
        "    \n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[:20]\n",
        "        frames = frames[None, ...]\n",
        "         # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :] )\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    print(frame_features.shape)\n",
        "\n",
        "    return frame_features\n",
        "\n",
        "        #frames_all.append(frames)\n",
        "\n",
        "    #return np.array(frames_all)\n"
      ],
      "metadata": {
        "id": "vgkwgo-m5dls"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cap = cv2.VideoCapture('/content/gdrive/MyDrive/ksutackle_dataset/risky_7.mp4')\n",
        "ret, frame = cap.read()\n",
        "i = 0\n",
        "\n",
        "while ret:\n",
        "  # try:\n",
        "  #   ret, frame = cap.read()\n",
        "  #   if not ret:\n",
        "  #     break\n",
        "    #cv2_imshow(frame)\n",
        "    #frame = crop_center(frame)\n",
        "    #cv2.imwrite(f'frame_{i}.jpg', frame)\n",
        "    ret, frame = cap.read()\n",
        "    i += 1\n",
        "    #cv2_imshow(frame)\n",
        "    #frame = frame[:,:,[1,2,0]]\n",
        "    #print(frame.shape)\n",
        "    #cv2_imshow(frame)\n",
        "\n",
        "\n",
        "  # finally:\n",
        "  #   cap.release()"
      ],
      "metadata": {
        "id": "TWGdExen5dps"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Kgxt2bl25dtu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axRgKahF4O8s",
        "outputId": "5d4706d5-cfc2-42af-b74a-9cbe15946e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data from disk\n",
            "Frame features in train set: (75, 20, 1024)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    data = np.load(\"data_conv_lstm.npy\")\n",
        "    print(\"Successfully loaded data from disk\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Dataset not available on disk, preparing a new one...\")\n",
        "    #data = prepare_all_videos('../../ksutackle_dataset_108')\n",
        "    data = prepare_all_videos('/content/gdrive/MyDrive/ksutackle_dataset')\n",
        "    np.save(\"data_conv_lstm.npy\", data)\n",
        "    #np.save(\"labels.npy\", labels)\n",
        "\n",
        "train_data_all, test_data  = train_test_split(data, test_size=0.30, random_state=42)\n",
        "#train_data, val_data, train_labels, val_labels = train_test_split(train_data_all, train_labels_all, test_size=0.20, random_state=45)\n",
        "print(f\"Frame features in train set: {train_data_all.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Normalize the data to the 0-1 range.\n",
        "train_dataset = train_data_all / 255\n",
        "val_dataset = test_data / 255\n",
        "\n",
        "# We'll define a helper function to shift the frames, where\n",
        "# `x` is frames 0 to n - 1, and `y` is frames 1 to n.\n",
        "def create_shifted_frames(data):\n",
        "    # x = data[:, 0 : data.shape[1] - 1, :, :]\n",
        "    # y = data[:, 1 : data.shape[1], :, :]\n",
        "    x = data[:, 0 : data.shape[1] - 1, :]\n",
        "    y = data[:, 1 : data.shape[1], :]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# Apply the processing function to the datasets.\n",
        "x_train, y_train = create_shifted_frames(train_dataset)\n",
        "x_val, y_val = create_shifted_frames(val_dataset)\n",
        "\n",
        "# Inspect the dataset.\n",
        "print(\"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape))\n",
        "print(\"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ehxgPZItx4",
        "outputId": "63c6834f-e64c-4809-8604-7609645187dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Shapes: (75, 19, 1024), (75, 19, 1024)\n",
            "Validation Dataset Shapes: (33, 19, 1024), (33, 19, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = layers.Reshape((-1,16,16,4))(y_train)\n",
        "\n",
        "x_val = layers.Reshape((-1,16,16,4))(x_val)\n",
        "y_val = layers.Reshape((-1,16,16,4))(y_val)"
      ],
      "metadata": {
        "id": "Fm76UhHXKOH6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VEFoXDe-KONi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"## Data Visualization\n",
        "Our data consists of sequences of frames, each of which\n",
        "are used to predict the upcoming frame. Let's take a look\n",
        "at some of these sequential frames.\n",
        "\"\"\"\n",
        "\n",
        "# Construct a figure on which we will visualize the images.\n",
        "fig, axes = plt.subplots(4, 5, figsize=(10, 8))\n",
        "\n",
        "# Plot each of the sequential images for one random data example.\n",
        "data_choice = np.random.choice(range(len(train_dataset)), size=1)[0]\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "    ax.imshow(np.squeeze(train_dataset[data_choice][idx]), cmap=\"gray\")\n",
        "    ax.set_title(f\"Frame {idx + 1}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Print information and display the figure.\n",
        "print(f\"Displaying frames for example {data_choice}.\")\n",
        "plt.show()\n",
        "\n",
        "\"\"\"## Model Construction\n",
        "To build a Convolutional LSTM model, we will use the\n",
        "`ConvLSTM2D` layer, which will accept inputs of shape\n",
        "`(batch_size, num_frames, width, height, channels)`, and return\n",
        "a prediction movie of the same shape.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "id": "cbaaYsZwIt1t",
        "outputId": "52aac428-1d48-4aed-99d2-870eec9caca6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9f722d3670bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdata_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_choice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Frame {idx + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    698\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 699\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1024,) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAHWCAYAAAA/0l4bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdf2xk9X3v/+f7i2Nok5Ysy7aKbEN2rtECG7kinqW5itQWtWIRSkykVMh81VsoiVa0l1ZKpUi0kbY35I/6qlL6FVqkBDUk6f0jTsI/u1GbRagE9Z+SxdvwIyYia+82WY/QBbIJapTKxpv39485C7Nee31sj33Gx8+HdOQ553zOzMe85r28fWbOTGQmkiRJ2lr/T9UTkCRJ2olswiRJkipgEyZJklQBmzBJkqQK2IRJkiRVwCZMkiSpAqWasIh4PCJei4jvr7A/IuKRiJiJiBcj4oMd++6NiFPFcm+3Jq71Mcv6MMt6Mc/6MEuVlpmrLsDvAB8Evr/C/juBbwMBfAj4brH9GuB08XNXcXtXmcd02ZzFLOuzmGW9FvOsz2KWLmWXUmfCMvNfgXOXGXIX8I/Z9izw3oh4H3AQeCozz2XmT4GngDvKPKY2h1nWh1nWi3nWh1mqrG69J2wAONuxPldsW2m7epdZ1odZ1ot51odZCoC+qidwQUQcAg4BvPvd7x698cYbK55RfX3gAx9gZmaGZrO53HdWLQATG7l/s9w6l8vy6quv5s033/ww7Zc11sUst5Z51odZ7hwnT558IzP3rOvgsq9bAu9n5de3vwjc07H+CvA+4B7giyuNW2kZHR1NbZ4zZ87k/v37l90HvG6W28flsjx06FACp9Mstw3zrA+z3DmAqdzM94SVcAz44+KKjw8Bb2bmq8CTwO0RsSsidgG3F9vUu36GWdbC2NgYwG6zrAfzrA+z1AWlXo6MiK8BvwdcGxFzwN8A7wLIzC8A/0z7ao8Z4BfAnxT7zkXE54Dnirt6ODMv92ZFbbJ77rmHZ555hjfeeIPBwUE++9nP8tZbbwHwwAMPALxJ+4ocs+xxq2V55513AsxjltuCedaHWaqsaJ9J6y3NZjOnpqaqnsaOFBEnM7PZrfszy2p1M0+zrJ551odZ1sdGsvQT8yVJkipgEyZJklQBmzBJkqQK2IRJkiRVwCZMkiSpAjZhkiRJFbAJkyRJqoBNmCRJUgVswiRJkipgEyZJklQBmzBJkqQK2IRJkiRVwCZMkiSpAjZhkiRJFbAJkyRJqkDpJiwi7oiIVyJiJiIeWmb/30fE88Xyw4j4Wce+8x37jnVr8lq748ePs2/fPoaHh5mYmFhuyJA5bh/mWR+rZfmpT30K4Gaz3B6sTZWSmasuwBXALNAA+oEXgJsvM/7Pgcc71n9e5nEuLKOjo6nuW1xczEajkbOzszk/P58jIyM5PT190RhgKruUY5rlptrqPM1y85TJMvOdPK3N3mZt7iydWa51KXsm7FZgJjNPZ+YCMAncdZnx9wBfK3nf2iInTpxgeHiYRqNBf38/4+PjHD169HKHmGMPM8/6MMt6MU+VVbYJGwDOdqzPFdsuERHXA3uBpzs2XxURUxHxbER8bF0z1Ya1Wi2GhobeXh8cHKTVai071hx7n3nWh1nWi3mqrL5NuM9x4InMPN+x7frMbEVEA3g6Il7KzNnOgyLiEHAI4LrrrtuEaWmN1pUjmGWPsi7rw9qsF2tzByt7JqwFDHWsDxbbljPOktOqmdkqfp4GngFuWXpQZj6Wmc3MbO7Zs6fktLQWAwMDnD37zgnNubk5BgaWPaEJ68yx2G+WW2Ar8jTLrWFt1ou1qbLKNmHPATdExN6I6Kf9pLnkio2IuBHYBfxbx7ZdEXFlcfta4MPAyxuduNbuwIEDnDp1ijNnzrCwsMDk5CRjY2OXjDPH7cE866NslsBVmGXPszZVVqkmLDMXgQeBJ4EfAN/IzOmIeDgiOp9Z48BkcbXABTcBUxHxAvAdYCIzfUJVoK+vjyNHjnDw4EFuuukm7r77bvbv38/hw4c5duyintoctwHzrI81ZHkNZtnzrE2VFRdn3xuazWZOTU1VPY0dKSJOZmazW/dnltXqZp5mWT3zrA+zrI+NZOkn5kuSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKlC6CYuIOyLilYiYiYiHltl/X0S8HhHPF8snO/bdGxGniuXebk1ea3f8+HH27dvH8PAwExMTyw3ZbY7bh3nWx2pZfuUrXwH4LbPcHqxNlZKZqy7AFcAs0AD6gReAm5eMuQ84ssyx1wCni5+7itu7Lvd4o6Ojqe5bXFzMRqORs7OzOT8/nyMjIzk9PX3RGOBMt3JMs9xUW52nWW6eMll++ctfTuC1tDZ7nrW5swBTWaKXWm4peybsVmAmM09n5gIwCdxV8tiDwFOZeS4zfwo8BdxR8lh10YkTJxgeHqbRaNDf38/4+DhHjx4te7g59hjzrA+zrBfzVFllm7AB4GzH+lyxbamPR8SLEfFERAyt8VhtslarxdDQ0Nvrg4ODtFqt5Yaa4zZgnvWxhizfa5a9z9pUWd18Y/63gPdn5gjtzv2razk4Ig5FxFRETL3++utdnJbW6GdsIEcwyx6zoTzNsnd89KMfBXjJ2qwNa1Olm7AWMNSxPlhse1tm/iQz54vVfwBGyx5bHP9YZjYzs7lnz56S09JaDAwMcPbsO39gzc3NMTBwyR9Y5zeSI5jlVtmKPM1ya5TJcvfu3QBZrFqbPczaVFllm7DngBsiYm9E9APjwLHOARHxvo7VMeAHxe0ngdsjYldE7AJuL7Zpix04cIBTp05x5swZFhYWmJycZGxsbOmwd3XcNsceZp71USbLV199tXPVLHuYtamy+soMyszFiHiQ9hPhCuDxzJyOiIdpXxVwDPiLiBgDFoFztK+WJDPPRcTnaDdyAA9n5rku/x4qoa+vjyNHjnDw4EHOnz/P/fffz/79+zl8+DDNZvPCPxK/ERHTmGPPM8/6KJPlI488ArA/Il7ALHuatamyon11ZW9pNps5NTVV9TR2pIg4mZnNbt2fWVarm3maZfXMsz7Msj42kqWfmC9JklQBmzBJkqQK2IRJkiRVwCZMkiSpAjZhkiRJFbAJkyRJqoBNmCRJUgVswiRJkipgEyZJklQBmzBJkqQK2IRJkiRVwCZMkiSpAjZhkiRJFbAJkyRJqoBNmCRJUgVKN2ERcUdEvBIRMxHx0DL7/zIiXo6IFyPiXyLi+o595yPi+WI51q3Ja+2OHz/Ovn37GB4eZmJiYrkhv2mO24d51sdqWX7+858H2G+W24O1qVIyc9UFuAKYBRpAP/ACcPOSMbcBv1rc/lPg6x37fl7mcS4so6Ojqe5bXFzMRqORs7OzOT8/nyMjIzk9PX3RGOCVbuWYZrmptjpPs9w8ZbJ8+umnE/j3tDZ7nrW5swBTucb6u7CUPRN2KzCTmaczcwGYBO5a0sx9JzN/Uaw+CwyWvG9tkRMnTjA8PEyj0aC/v5/x8XGOHj26dNh/muP2YJ71USbL2267DeCXxapZ9jBrU2WVbcIGgLMd63PFtpV8Avh2x/pVETEVEc9GxMfWOEd1SavVYmho6O31wcFBWq3W5Q4xxx5mnvVhlvViniqrr9t3GBF/BDSB3+3YfH1mtiKiATwdES9l5uyS4w4BhwCuu+66bk9La7TeHItjzbLHWJf1YW3Wi7W5s5U9E9YChjrWB4ttF4mIPwA+A4xl5vyF7ZnZKn6eBp4Bbll6bGY+lpnNzGzu2bOn9C+g8gYGBjh79p0TmnNzcwwMXHpCcyM5FvvNcgtsRZ5muTXKZgn8GtZmz7M2VVbZJuw54IaI2BsR/cA4cNEVGxFxC/BF2k+m1zq274qIK4vb1wIfBl7uxuS1NgcOHODUqVOcOXOGhYUFJicnGRsbWzrsVzDHbcE866NMlt/73vcArscse561qbJKvRyZmYsR8SDwJO0rJR/PzOmIeJj2VQHHgL8D3gN8MyIAfpyZY8BNwBcj4pe0m76JzPQJVYG+vj6OHDnCwYMHOX/+PPfffz/79+/n8OHDNJvNC/9IDAH/hTn2PPOsjzJZfvrTn4b2v79m2eOsTZUV7asre0uz2cypqamqp7EjRcTJzGx26/7MslrdzNMsq2ee9WGW9bGRLP3EfEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKlC6CYuIOyLilYiYiYiHltl/ZUR8vdj/3Yh4f8e+vyq2vxIRB7szda3X8ePH2bdvH8PDw0xMTFyy3yy3j9WyBMIst4fVspyfnwdomOX2YG2qlMxcdQGuAGaBBtAPvADcvGTMnwFfKG6PA18vbt9cjL8S2FvczxWXe7zR0dHU5lhcXMxGo5Gzs7M5Pz+fIyMjOT09/fZ+YMost4fVsszMBH5klr2vTJaPPvpoAq9lF7JM89xU1ubOAkxliV5quaXsmbBbgZnMPJ2ZC8AkcNeSMXcBXy1uPwH8fkREsX0yM+cz8wwwU9yfKnDixAmGh4dpNBr09/czPj7O0aNHlw4zy22gZJbvxSx7Xpksi/WfFKtm2cOsTZXVV3LcAHC2Y30O+O2VxmTmYkS8Cewutj+75NiBpQ8QEYeAQ8XqfER8v+TcetG1wBtVT2IFu4Bfj4gfFevXAO/567/+6x8X6/to/4Vmlm3bOUuAWzDLC7Z7lvuBIVhflmCeW8jaXJtezrKMfes9sGwTtuky8zHgMYCImMrMZsVTWrdenn9E/CFwR2Z+slj/H8BvZ+aDxfoUcNVGHsMst8ZqWRbb/msjj2GWW6Nklt8Hzm/kccxza1iba1OH+a/32LIvR7Yo/gIrDBbblh0TEX3A1bRPnZc5VlvHLOujTB4LmOV2ULYu+8EstwFrU6WUbcKeA26IiL0R0U/7TYTHlow5Btxb3P5D4OniDWvHgPHiiru9wA3AiY1PXetklvVRJsufYZbbQdm63F3cNsveZm2qlFIvRxavVz8IPEn7SsnHM3M6Ih6mfVXAMeBLwP+JiBngHO0nHcW4bwAvA4vA/8zM1U6pP7a+X6dn9Oz8V8uS9tz/EbO8oGfnX7Iu/zdwm1kCPTz/Nfwb+yddyhJ6+L9HST07f2tzzXbs/KPdeEuSJGkr+Yn5kiRJFbAJkyRJqkClTVhs4KuQqlZi7vdFxOsR8XyxfLKKeS4nIh6PiNdW+lyZaHuk+N1ejIgPlrjPbZslmOeS8WZZIfO82HbO0ywvtp2zhM35f+e6Pma/Gwsb+CqkqpeSc78POFL1XFeY/+8AHwS+v8L+O4FvAwF8CPhuXbM0T7PstcU865OnWdYny27neWGp8kzYRr4KqWpl5t6zMvNfaV+Ns5K7gH/MtmeB90bE+y4zfjtnCebZySwrZp4X2dZ5muVFtnWWsCn/76y0CVvuq5CWfjXDRV+FBFz4WoeqlZk7wMeLU5JPRMTQMvt7Vdnfby3jezVLMM+1jjXLapnnpbZrnmZ5qe2aJaz9/52+MX8TfQt4f2aOAE/xzl8n2p7Msz7Msl7Msz52XJZVNmEb+fqcqq0698z8SWbOF6v/AIxu0dy6Ya1fm7GdswTzXOtYs6yWeXbY5nmaZYdtniWs4yunqmzCNvL1OVVbde5LXgceA36whfPbqGPAHxdXenwIeDMzX73M+O2cJZhnJ7PsfebZYZvnaZYdtnmWsPb/d1Z3dWTHlQQ/pH3FxGeKbQ8DY8Xtq4BvAjO0vzurUeV81zj3vwWmaV8B8h3gxqrn3DH3rwGvAm/Rfs36E8ADwAPF/gAeLX63l4BmnbM0T7Oses7mWc88zbI+WW5Gnpm5ehMGPA68xsqXZAbwSBH6i8AHO/bdC5wqlnur/g/oYp51WsyyXot51mcxS5fSz5VVB6zzczGAa4DTxc9dxe1dVf/CO30xz/osZlmvxTzrs5ilS9ll1feE5fo/F+Mg8FRmnsvMn9K+0uGO1R5Pm8s868Ms68U868MsVVZfF+5jpc/FKP15GRFxCDgE8O53v3v0xhtv7MK0tJIPfOADzMzM0Gw2L3nD5tVXX82bb755N+3XvsE8e5pZ1ot51odZ7hwnT558IzP3rOfYbjRhG5aZjwGPATSbzZyamqp4RvX2H//xH3zkIx9huf/OH/nIR/inf/qn/7uR+zfPrWOW9WKe9WGWO0dE/Gi9x3bjIypW+lyMNX9ehqo3MDAA7e/1usA8tymzrBfzrA+z1AXdaMJW+lyMJ4HbI2JXROwCbi+2qYeNjY0B7DbP7c8s68U868MsdcGqL0dGxNeA3wOujYg54G+AdwFk5heAf6Z9pccM8AvgT4p95yLic7Q/oA3g4cy83BsVtQXuuecennnmGd544w0GBwf57Gc/y1tvvQXAAw88wJ133gkwj3n2PLOsF/OsD7NUWZHZKx+m2+Zr29WLiJOZ2ezGfZlntcyyXsyzPsyyPjaSpV/gLUmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqQKkmLCLuiIhXImImIh5aZv/fR8TzxfLDiPhZx77zHfuOdXPyWrvjx4+zb98+hoeHmZiYuGT/pz71KYCbzXJ7WC1PYMja3B6szXqxNlVKZl52Aa4AZoEG0A+8ANx8mfF/Djzesf7z1R6jcxkdHU1tjsXFxWw0Gjk7O5vz8/M5MjKS09PTl4wDprILWaZ5bqoyeV7IMq3NnmZt1ou1ubN0ZrnWpcyZsFuBmcw8nZkLwCRw12XG3wN8rcT9aoudOHGC4eFhGo0G/f39jI+Pc/To0csdYpY9zDzrwyzrxTxVVpkmbAA427E+V2y7RERcD+wFnu7YfFVETEXEsxHxsXXPVBvWarUYGhp6e31wcJBWq7XsWLPsfeZZH2ZZL+apsvq6fH/jwBOZeb5j2/WZ2YqIBvB0RLyUmbOdB0XEIeAQwHXXXdflKWmd1pUlmGePsjbrw9qsF2tzBytzJqwFDHWsDxbbljPOklOqmdkqfp4GngFuWXpQZj6Wmc3MbO7Zs6fElLQeAwMDnD37zknNubk5BgaWPakJ68yy2G+eW2Ar8jTLrWFt1ou1qbLKNGHPATdExN6I6Kf9hLnkao2IuBHYBfxbx7ZdEXFlcfta4MPAy92YuNbuwIEDnDp1ijNnzrCwsMDk5CRjY2PLDb0Ks+x5ZfO0NnuftVkv1qbKWrUJy8xF4EHgSeAHwDcyczoiHo6IzmfVODBZXClwwU3AVES8AHwHmMhMn0wV6evr48iRIxw8eJCbbrqJu+++m/3793P48GGOHbuor74Gs+x5a8jT2uxx1ma9WJsqKy7OvnrNZjOnpqaqnsaOFhEnM7PZjfsyz2qZZb2YZ32YZX1sJEs/MV+SJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpAqUasIi4o6IeCUiZiLioWX23xcRr0fE88XyyY5990bEqWK5t5uT19odP36cffv2MTw8zMTExCX7v/KVrwD8llluD6vlCey2NrcHa7NerE2VkpmXXYArgFmgAfQDLwA3LxlzH3BkmWOvAU4XP3cVt3dd7vFGR0dTm2NxcTEbjUbOzs7m/Px8joyM5PT09EVjvvzlLyfwWnYhyzTPTVUmT+CMtdn7rM16sTZ3FmAqV6m3lZYyZ8JuBWYy83RmLgCTwF0le7yDwFOZeS4zfwo8BdxR8lh12YkTJxgeHqbRaNDf38/4+DhHjx4te7hZ9hjzrA+zrBfzVFllmrAB4GzH+lyxbamPR8SLEfFERAyt8VhtgVarxdDQ0Nvrg4ODtFqt5Ya+1yx73xrytDZ7nLVZL9amyurWG/O/Bbw/M0dod+1fXcvBEXEoIqYiYur111/v0pS0Hh/96EcBXlpvlmCePeZnWJu1YG3WjrWpUk1YCxjqWB8str0tM3+SmfPF6j8Ao2WPLY5/LDObmdncs2dP2blrjQYGBjh79p0/sObm5hgYuPgPrN27dwNksbrmLME8t0qZPIHz1mbvszbrxdpUWWWasOeAGyJib0T0A+PAsc4BEfG+jtUx4AfF7SeB2yNiV0TsAm4vtqkCBw4c4NSpU5w5c4aFhQUmJycZGxu7aMyrr77auWqWPaxMnsC7Om6bZ4+yNuvF2lRZfasNyMzFiHiQ9pPgCuDxzJyOiIdpXxFwDPiLiBgDFoFztK+WJDPPRcTnaDdyAA9n5rlN+D1UQl9fH0eOHOHgwYOcP3+e+++/n/3793P48GGazSZjY2M88sgjAPsj4gXMsqeVyRP4jYiYxtrsadZmvVibKivaV1f2jmazmVNTU1VPY0eLiJOZ2ezGfZlntcyyXsyzPsyyPjaSpZ+YL0mSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBUo1YRFxR0S8EhEzEfHQMvv/MiJejogXI+JfIuL6jn3nI+L5YjnWzclr7Y4fP86+ffsYHh5mYmLikv2f//znAfab5fawWp7Ab1qb24O1WS/WpkrJzMsuwBXALNAA+oEXgJuXjLkN+NXi9p8CX+/Y9/PVHqNzGR0dTW2OxcXFbDQaOTs7m/Pz8zkyMpLT09MXjXn66acT+PfsQpZpnpuqTJ7AK9Zm77M268Xa3FmAqVxj/V1YypwJuxWYyczTmbkATAJ3LWnkvpOZvyhWnwUGyzSA2lonTpxgeHiYRqNBf38/4+PjHD169KIxt912G8Avi1Wz7GFl8gT+09rsfdZmvVibKqtMEzYAnO1Ynyu2reQTwLc71q+KiKmIeDYiPrbcARFxqBgz9frrr5eYktaj1WoxNDT09vrg4CCtVutyh6w5SzDPrbIVeZrl1rA268XaVFl93byziPgjoAn8bsfm6zOzFREN4OmIeCkzZzuPy8zHgMcAms1mdnNOWp/1Zgnm2YuszfqwNuvF2tzZypwJawFDHeuDxbaLRMQfAJ8BxjJz/sL2zGwVP08DzwC3bGC+2oCBgQHOnn3npObc3BwDA8ue1Pw1zLLnlc3T2ux91ma9WJsqq0wT9hxwQ0TsjYh+YBy46GqNiLgF+CLtJ9JrHdt3RcSVxe1rgQ8DL3dr8lqbAwcOcOrUKc6cOcPCwgKTk5OMjY1dNOZ73/sewPWYZc8rkyfwK1ibPc/arBdrU2Wt+nJkZi5GxIPAk7SvlHw8M6cj4mHaVwQcA/4OeA/wzYgA+HFmjgE3AV+MiF/SbvgmMtMnU0X6+vo4cuQIBw8e5Pz589x///3s37+fw4cP02w2GRsb49Of/jS0czbLHlcmT9pnsf8L8+xp1ma9WJsqK9pXV/aOZrOZU1NTVU9jR4uIk5nZ7MZ9mWe1zLJezLM+zLI+NpKln5gvSZJUAZswSZKkCtiESZIkVcAmTJIkqQI2YZIkSRWwCZMkSaqATZgkSVIFbMIkSZIqYBMmSZJUAZswSZKkCtiESZIkVcAmTJIkqQI2YZIkSRWwCZMkSapAqSYsIu6IiFciYiYiHlpm/5UR8fVi/3cj4v0d+/6q2P5KRBzs3tS1XsePH2ffvn0MDw8zMTGx3JAwz+3BLOtjtSzn5+cBGma5PVibKiUzL7sAVwCzQAPoB14Abl4y5s+ALxS3x4GvF7dvLsZfCewt7ueKyz3e6OhoavMsLi5mo9HI2dnZnJ+fz5GRkZyenr5oDPAj8+x9ZlkfZbJ89NFHE3gtu5BlmuemsjZ3FmAqV6m3lZYyZ8JuBWYy83RmLgCTwF1LxtwFfLW4/QTw+xERxfbJzJzPzDPATHF/qsiJEycYHh6m0WjQ39/P+Pg4R48eXTrsvZhnzzPL+iiTZbH+k2LVLHuYtamyyjRhA8DZjvW5YtuyYzJzEXgT2F3yWG2hVqvF0NDQ2+uDg4O0Wq2lw/oxz55nlvVRJstifQHMstdZmyqrr+oJAETEIeBQsTofEd+vcj5dcC3wRtWTWMEu4Ne/9KUv/ahYvwZ4z6OPPvrjjjG3bOQBapanWZrlViiT5X7gv23kQcxzy1iba9PLWZaxb70HlmnCWsBQx/pgsW25MXMR0QdcTfu0eZljyczHgMcAImIqM5tlf4Fe1Mu/Q0T8d+B/ZebBYv2vADLzbzvGvIl5Ar09f7Ncm16ef8ksnwSuK26vOcvi/sxzC1iba1OH+a/32DIvRz4H3BAReyOin/YbCI8tGXMMuLe4/YfA08Wb1Y4B48XVk3uBG4AT652suqJMnj/DPLcDs6yPsv/O7i5um2VvszZVyqpnwjJzMSIeBJ6kfaXk45k5HREP074i4BjwJeD/RMQMcI72E45i3DeAl4FF4H9m5vlN+l1UQsk83wB2m2dvM8v6WMO/s58zy95nbaqsaDfevSMiDhWnWbet7f47dHP+/reollm+Y7vPH8yzk/PfnPuqwk6ef881YZIkSTuBX1skSZJUgcqasNjAVyH1ghLzvy8iXo+I54vlk1XMcyUR8XhEvLbSZc3R9kjx+70YER9c5f7MsyJmebHtnCWY51LbOU+zvNh2zhK6nyew+tcWbcbCBr4KqReWkvO/DzhS9Vwv8zv8DvBB4Psr7L8T+DYQwIeA75pnb+ZplvXJ0jzrladZ1ifLbud5YanqTNhGvgqpF5SZf0/LzH+lfUXOSu4C/jHbngXeGxHvW2GseVbILC+yrbME81xiW+dplhfZ1llC1/MEqns5ciNfhdQLyn6txMeLU5JPRMTQMvt72Vq+OsM8e5tZXmq7ZgnmWac8zfJS2zVLWMdXTvnG/M3zLeD9mTkCPMU7f9j3YuUAACAASURBVJ1oezLP+jDLejHP+thxWVbVhK3lq5CWfkVHL1h1/pn5k8ycL1b/ARjdorl1S+mvQik51jyrY5YdtnmWYJ51ytMsO2zzLGFteQLVNWEb+SqkXrDq/Je8DjwG/GAL59cNx4A/Lq72+BDwZma+usJY8+xtZtlhm2cJ5lmnPM2ywzbPEtaWZ1uFVxncCfyQ9tUSnym2PQyMFbevAr4JzND+3qxGVXNd5/z/FpimfQXId4Abq57zkvl/DXgVeIv269afAB4AHij2B/Bo8fu9BDTNszfzNMv6ZGme9crTLOuT5WbkmZmrN2HA48BrrHxJZgCPFKG/CHywY9+9wKliubfq/4Au5lmnxSzrtZhnfRazdCn9XFl1wDo/FwO4Bjhd/NxV3N5V9S+80xfzrM9ilvVazLM+i1m6lF1WfU9Yrv9zMQ4CT2Xmucz8Ke0rHe5Y7fG0ucyzPsyyXsyzPsxSZXXjjfkrfS7Gmj8vQz3BPOvDLOvFPOvDLAVAX9UTAIiIQ8AhgHe/+92jN954Y8UzqrcPfOADzMzM0Gw2L7lq5uqrr+bNN9/8MO1T4etinlvHLOvFPOvDLHeOkydPvpGZe9Z1cJnXLIH3s/Jr218E7ulYfwV4H3AP8MWVxq20jI6OpjbXmTNncv/+/cvuO3ToUAKn0zy3BbOsF/OsD7PcOYCp3Kz3hJWw0udiPAncHhG7ImIXcHuxTT1sbGwMYLd5bn9mWS/mWR9mqQtWfTkyIr4G/B5wbUTMAX8DvAsgM78A/DPtKz1mgF8Af1LsOxcRn6P9AW0AD2fm5d6oqC1wzz338Mwzz/DGG28wODjIZz/7Wd566y0AHnjgAe68806Aecyz55llvZhnfZilyor2mbTe0Ww2c2pqqupp7GgRcTIzm924L/OsllnWi3nWh1nWx0ay9Au8JUmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqQKkmLCLuiIhXImImIh5aZv/fR8TzxfLDiPhZx77zHfuOdXPyWrvjx4+zb98+hoeHmZiYuGT/pz71KYCbzXJ7WC1PYMja3B6szXqxNlVKZl52Aa4AZoEG0A+8ANx8mfF/Djzesf7z1R6jcxkdHU1tjsXFxWw0Gjk7O5vz8/M5MjKS09PTl4wDprILWaZ5bqoyeV7IMq3NnmZt1ou1ubN0ZrnWpcyZsFuBmcw8nZkLwCRw12XG3wN8rcT9aoudOHGC4eFhGo0G/f39jI+Pc/To0csdYpY9zDzrwyzrxTxVVpkmbAA427E+V2y7RERcD+wFnu7YfFVETEXEsxHxsRWOO1SMmXr99ddLTl1r1Wq1GBoaent9cHCQVqu17Nj1Zlkca55bYCvyNMutYW3Wi7Wpsrr9xvxx4InMPN+x7frMbAL/L/D/RcR/W3pQZj6Wmc3MbO7Zs6fLU9I6rStLMM8eZW3Wh7VZL9bmDlamCWsBQx3rg8W25Yyz5JRqZraKn6eBZ4Bb1jxLdcXAwABnz75zUnNubo6BgWVPaoJZ9jzzrA+zrBfzVFllmrDngBsiYm9E9NN+wlxytUZE3AjsAv6tY9uuiLiyuH0t8GHg5W5MXGt34MABTp06xZkzZ1hYWGBycpKxsbHlhl6FWfa8snlam73P2qwXa1NlrdqEZeYi8CDwJPAD4BuZOR0RD0dE57NqHJgsrhS44CZgKiJeAL4DTGSmT6aK9PX1ceTIEQ4ePMhNN93E3Xffzf79+zl8+DDHjl3UV1+DWfa8NeRpbfY4a7NerE2VFRdnX71ms5lTU1NVT2NHi4iTxfsRNsw8q2WW9WKe9WGW9bGRLP3EfEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBUo1YRFxR0S8EhEzEfHQMvvvi4jXI+L5Yvlkx757I+JUsdzbzclr7Y4fP86+ffsYHh5mYmLikv1f+cpXAH7LLLeH1fIEdlub24O1WS/WpkrJzMsuwBXALNAA+oEXgJuXjLkPOLLMsdcAp4ufu4rbuy73eKOjo6nNsbi4mI1GI2dnZ3N+fj5HRkZyenr6ojFf/vKXE3gtu5BlmuemKpMncMba7H3WZr1YmzsLMJWr1NtKS5kzYbcCM5l5OjMXgEngrpI93kHgqcw8l5k/BZ4C7ih5rLrsxIkTDA8P02g06O/vZ3x8nKNHj5Y93Cx7jHnWh1nWi3mqrDJN2ABwtmN9rti21Mcj4sWIeCIihtZ4rLZAq9ViaGjo7fXBwUFardZyQ99rlr1vDXlamz3O2qwXa1NldeuN+d8C3p+ZI7S79q+u5eCIOBQRUxEx9frrr3dpSlqPj370owAvrTdLMM8e8zOszVqwNmvH2lSpJqwFDHWsDxbb3paZP8nM+WL1H4DRsscWxz+Wmc3MbO7Zs6fs3LVGAwMDnD37zh9Yc3NzDAxc/AfW7t27AbJYXXOWYJ5bpUyewHlrs/dZm/VibaqsMk3Yc8ANEbE3IvqBceBY54CIeF/H6hjwg+L2k8DtEbErInYBtxfbVIEDBw5w6tQpzpw5w8LCApOTk4yNjV005tVXX+1cNcseViZP4F0dt82zR1mb9WJtqqy+1QZk5mJEPEj7SXAF8HhmTkfEw7SvCDgG/EVEjAGLwDnaV0uSmeci4nO0GzmAhzPz3Cb8Hiqhr6+PI0eOcPDgQc6fP8/999/P/v37OXz4MM1mk7GxMR555BGA/RHxAmbZ08rkCfxGRExjbfY0a7NerE2VFe2rK3tHs9nMqampqqexo0XEycxsduO+zLNaZlkv5lkfZlkfG8nST8yXJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCpZqwiLgjIl6JiJmIeGiZ/X8ZES9HxIsR8S8RcX3HvvMR8XyxHOvm5LV2x48fZ9++fQwPDzMxMXHJ/s9//vMA+81ye1gtT+A3rc3twdqsF2tTpWTmZRfgCmAWaAD9wAvAzUvG3Ab8anH7T4Gvd+z7+WqP0bmMjo6mNsfi4mI2Go2cnZ3N+fn5HBkZyenp6YvGPP300wn8e3YhyzTPTVUmT+AVa7P3WZv1Ym3uLMBUrrH+LixlzoTdCsxk5unMXAAmgbuWNHLfycxfFKvPAoNlGkBtrRMnTjA8PEyj0aC/v5/x8XGOHj160ZjbbrsN4JfFqln2sDJ5Av9pbfY+a7NerE2VVaYJGwDOdqzPFdtW8gng2x3rV0XEVEQ8GxEfW8cc1SWtVouhoaG31wcHB2m1Wpc7xCx7mHnWh1nWi3mqrL5u3llE/BHQBH63Y/P1mdmKiAbwdES8lJmzS447BBwCuO6667o5Ja3TerMsjjXPHmNt1oe1WS/W5s5W5kxYCxjqWB8stl0kIv4A+AwwlpnzF7ZnZqv4eRp4Brhl6bGZ+VhmNjOzuWfPnjX9AipvYGCAs2ffOak5NzfHwMCyJzV/jXVmWew3zy1QNk9rs/dZm/VibaqsMk3Yc8ANEbE3IvqBceCiqzUi4hbgi7SfSK91bN8VEVcWt68FPgy83K3Ja20OHDjAqVOnOHPmDAsLC0xOTjI2NnbRmO9973sA12OWPa9MnsCvYG32PGuzXqxNlbXqy5GZuRgRDwJP0r5S8vHMnI6Ih2lfEXAM+DvgPcA3IwLgx5k5BtwEfDEifkm74ZvITJ9MFenr6+PIkSMcPHiQ8+fPc//997N//34OHz5Ms9lkbGyMT3/609DO2Sx7XJk8aZ/F/i/Ms6dZm/VibaqsaF9d2TuazWZOTU1VPY0dLSJOZmazG/dlntUyy3oxz/owy/rYSJZ+Yr4kSVIFbMIkSZIqYBMmSZJUAZswSZKkCtiESZIkVcAmTJIkqQI2YZIkSRWwCZMkSaqATZgkSVIFbMIkSZIqYBMmSZJUAZswSZKkCtiESZIkVcAmTJIkqQI2YZIkSRUo1YRFxB0R8UpEzETEQ8vsvzIivl7s/25EvL9j318V21+JiIPdm7rW6/jx4+zbt4/h4WEmJiaWGxLmuT2YZX2sluX8/DxAwyy3B2tTpWTmZRfgCmAWaAD9wAvAzUvG/BnwheL2OPD14vbNxfgrgb3F/VxxuccbHR1NbZ7FxcVsNBo5Ozub8/PzOTIyktPT0xeNAX5knr3PLOujTJaPPvpoAq9lF7JM89xU1ubOAkzlKvW20lLmTNitwExmns7MBWASuGvJmLuArxa3nwB+PyKi2D6ZmfOZeQaYKe5PFTlx4gTDw8M0Gg36+/sZHx/n6NGjS4e9F/PseWZZH2WyLNZ/UqyaZQ+zNlVWX4kxA8DZjvU54LdXGpOZixHxJrC72P7skmMHlj5ARBwCDhWr8xHx/VKz713XAm9UPYkV7AJ+PSJ+VKxfA7znr//6r3/cMeYWzPMCszTLrVAmy/3AEKwvSzDPLWRtrk0vZ1nGvvUeWKYJ23SZ+RjwGEBETGVms+IpbUgv/w4R8YfAHZn5yWL9fwC/nZkPdoz5r408Rp3y7OX5m+Xa9PL8S2b5feD8Rh7HPLeGtbk2dZj/eo8t83Jki+Kvr8JgsW3ZMRHRB1xN+7R5mWO1tcpksoB5bgdmWR9l/53tB7PcBqxNlVKmCXsOuCEi9kZEP+03EB5bMuYYcG9x+w+Bp4s3qx0DxourJ/cCNwAnujN1rVOZPH+GeW4HZlkfZf+d3V3cNsveZm2qlFVfjixeq34QeJL2lZKPZ+Z0RDxM+4qAY8CXgP8TETPAOdpPOIpx3wBeBhaB/5mZq51Of2z9v07P6NnfoWSe/xu4zTyBHp6/Wa5Zz85/Df/O/kmXsoQe/u9RUs/O39pcsx07/2g33pIkSdpKfmK+JElSBWzCJEmSKlBZExYb+CqkXlBi/vdFxOsR8XyxfLKKea4kIh6PiNdW+myZaHuk+P1ejIgPrnJ/5lkRs7zYds4SzHOp7ZynWV5sO2cJ3c8TWP1rizZjYQNfhdQLS8n53wccqXqul/kdfgf4IPD9FfbfCXwbCOBDwHfNszfzNMv6ZGme9crTLOuTZbfzvLBUdSZsI1+F1AvKzL+nZea/0r4iZyV3Af+Ybc8C742I960w1jwrZJYX2dZZgnkusa3zNMuLbOssoet5AtW9HLncVyEt/VqGi74KCbjwlQ69oMz8AT5enJJ8IiKGltnfy8r+jmXHmmd1zPJS2zVLMM865WmWl9quWcLa8gR8Y/5m+hbw/swcAZ7inb9OtD2ZZ32YZb2YZ33suCyrasI28lVIvWDV+WfmTzJzvlj9B2B0i+bWLWv56gzz7G1m2WGbZwnmWac8zbLDNs8S1vGVU1U1YRv5KqResOr8l7wOPAb8YAvn1w3HgD8urvb4EPBmZr66wljz7G1m2WGbZwnmWac8zbLDNs8S1pZnW4VXGdwJ/JD21RKfKbY9DIwVt68CvgnM0P7erEZVc13n/P8WmKZ9Bch3gBurnvOS+X8NeBV4i/br1p8AHgAeKPYH8Gjx+70ENM2zN/M0y/pkaZ71ytMs65PlZuSZmas3YcDjwGusfElmAI8Uob8IfLBj373AqWK5t+r/gC7mWafFLOu1mGd9FrN0Kf1cWXXAOj8XA7gGOF383FXc3lX1L7zTF/Osz2KW9VrMsz6LWbqUXVZ9T1iu/3MxDgJPZea5zPwp7Ssd7ljt8bS5zLM+zLJezLM+zFJl9XXhPlb6XIzSn5cREYeAQwDvfve7R2+88cYuTEsr+cAHPsDMzAzNZvOSN2xeffXVvPnmm3fTfu0bzLOnmWW9mGd9mOXOcfLkyTcyc896ju1GE7ZhmfkY8BhAs9nMqampimdUb//xH//BRz7yEZb77/yRj3yEf/qnf/q/G7l/89w6Zlkv5lkfZrlzRMSP1ntsNz6iYqXPxVjz52WoegMDA9D+Xq8LzHObMst6Mc/6MEtd0I0mbKXPxXgSuD0idkXELuD2Ypt62NjYGMBu89z+zLJezLM+zFIXrPpyZER8Dfg94NqImAP+BngXQGZ+Afhn2ld6zAC/AP6k2HcuIj5H+wPaAB7OzMu9UVFb4J577uGZZ57hjTfeYHBwkM9+9rO89dZbADzwwAPceeedAPOYZ88zy3oxz/owS5UVmb3yYbptvrZdvYg4mZnNbtyXeVbLLOvFPOvDLOtjI1n6Bd6SJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpAqUasIi4o6IeCUiZiLioWX2/31EPF8sP4yIn3XsO9+x71g3J6+1O378OPv27WN4eJiJiYlL9n/qU58CuNkst4fV8gSGrM3twdqsF2tTpWTmZRfgCmAWaAD9wAvAzZcZ/+fA4x3rP1/tMTqX0dHR1OZYXFzMRqORs7OzOT8/nyMjIzk9PX3JOGAqu5BlmuemKpPnhSzT2uxp1ma9WJs7S2eWa13KnAm7FZjJzNOZuQBMAnddZvw9wNdK3K+22IkTJxgeHqbRaNDf38/4+DhHjx693CFm2cPMsz7Msl7MU2WVacIGgLMd63PFtktExPXAXuDpjs1XRcRURDwbER9b90y1Ya1Wi6GhobfXBwcHabVay441y95nnvVhlvViniqrr8v3Nw48kZnnO7Zdn5mtiGgAT0fES5k523lQRBwCDgFcd911XZ6S1mldWYJ59ihrsz6szXqxNnewMmfCWsBQx/pgsW054yw5pZqZreLnaeAZ4JalB2XmY5nZzMzmnj17SkxJ6zEwMMDZs++c1Jybm2NgYNmTmrDOLIv95rkFtiJPs9wa1ma9WJsqq0wT9hxwQ0TsjYh+2k+YS67WiIgbgV3Av3Vs2xURVxa3rwU+DLzcjYlr7Q4cOMCpU6c4c+YMCwsLTE5OMjY2ttzQqzDLnlc2T2uz91mb9WJtqqxVm7DMXAQeBJ4EfgB8IzOnI+LhiOh8Vo0Dk8WVAhfcBExFxAvAd4CJzPTJVJG+vj6OHDnCwYMHuemmm7j77rvZv38/hw8f5tixi/rqazDLnreGPK3NHmdt1ou1qbLi4uyr12w2c2pqqupp7GgRcTIzm924L/OsllnWi3nWh1nWx0ay9BPzJUmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqQKkmLCLuiIhXImImIh5aZv99EfF6RDxfLJ/s2HdvRJwqlnu7OXmt3fHjx9m3bx/Dw8NMTExcsv8rX/kKwG+Z5fawWp7Abmtze7A268XaVCmZedkFuAKYBRpAP/ACcPOSMfcBR5Y59hrgdPFzV3F71+Ueb3R0NLU5FhcXs9Fo5OzsbM7Pz+fIyEhOT09fNObLX/5yAq9lF7JM89xUZfIEzlibvc/arBdrc2cBpnKVeltpKXMm7FZgJjNPZ+YCMAncVbLHOwg8lZnnMvOnwFPAHSWPVZedOHGC4eFhGo0G/f39jI+Pc/To0bKHm2WPMc/6MMt6MU+VVaYJGwDOdqzPFduW+nhEvBgRT0TE0BqP1RZotVoMDQ29vT44OEir1Vpu6HvNsvetIU9rs8dZm/Vibaqsbr0x/1vA+zNzhHbX/tW1HBwRhyJiKiKmXn/99S5NSevx0Y9+FOCl9WYJ5tljfoa1WQvWZu1YmyrVhLWAoY71wWLb2zLzJ5k5X6z+AzBa9tji+Mcys5mZzT179pSdu9ZoYGCAs2ff+QNrbm6OgYGL/8DavXs3QBara84SzHOrlMkTOG9t9j5rs16sTZVVpgl7DrghIvZGRD8wDhzrHBAR7+tYHQN+UNx+Erg9InZFxC7g9mKbKnDgwAFOnTrFmTNnWFhYYHJykrGxsYvGvPrqq52rZtnDyuQJvKvjtnn2KGuzXqxNldW32oDMXIyIB2k/Ca4AHs/M6Yh4mPYVAceAv4iIMWAROEf7akky81xEfI52IwfwcGae24TfQyX09fVx5MgRDh48yPnz57n//vvZv38/hw8fptlsMjY2xiOPPAKwPyJewCx7Wpk8gd+IiGmszZ5mbdaLtamyon11Ze9oNps5NTVV9TR2tIg4mZnNbtyXeVbLLOvFPOvDLOtjI1n6ifmSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpArYhEmSJFXAJkySJKkCNmGSJEkVsAmTJEmqgE2YJElSBWzCJEmSKmATJkmSVAGbMEmSpAqUasIi4o6IeCUiZiLioWX2/2VEvBwRL0bEv0TE9R37zkfE88VyrJuT19odP36cffv2MTw8zMTExCX7P//5zwPsN8vtYbU8gd+0NrcHa7NerE2VkpmXXYArgFmgAfQDLwA3LxlzG/Crxe0/Bb7ese/nqz1G5zI6OpraHIuLi9loNHJ2djbn5+dzZGQkp6enLxrz9NNPJ/Dv2YUs0zw3VZk8gVeszd5nbdaLtbmzAFO5xvq7sJQ5E3YrMJOZpzNzAZgE7lrSyH0nM39RrD4LDJZpALW1Tpw4wfDwMI1Gg/7+fsbHxzl69OhFY2677TaAXxarZtnDyuQJ/Ke12fuszXqxNlVWmSZsADjbsT5XbFvJJ4Bvd6xfFRFTEfFsRHxsHXNUl7RaLYaGht5eHxwcpNVqXe4Qs+xh5lkfZlkv5qmy+rp5ZxHxR0AT+N2OzddnZisiGsDTEfFSZs4uOe4QcAjguuuu6+aUtE7rzbI41jx7jLVZH9ZmvVibO1uZM2EtYKhjfbDYdpGI+APgM8BYZs5f2J6ZreLnaeAZ4Jalx2bmY5nZzMzmnj171vQLqLyBgQHOnn3npObc3BwDA8ue1Pw11pllsd88t0DZPK3N3mdt1ou1qbLKNGHPATdExN6I6AfGgYuu1oiIW4Av0n4ivdaxfVdEXFncvhb4MPBytyavtTlw4ACnTp3izJkzLCwsMDk5ydjY2EVjvve97wFcj1n2vDJ5Ar+CtdnzrM16sTZV1qovR2bmYkQ8CDxJ+0rJxzNzOiIepn1FwDHg74D3AN+MCIAfZ+YYcBPwxYj4Je2GbyIzfTJVpK+vjyNHjnDw4EHOnz/P/fffz/79+zl8+DDNZpOxsTE+/elPQztns+xxZfKkfRb7vzDPnmZt1ou1qbKifXVl72g2mzk1NVX1NHa0iDiZmc1u3Jd5Vsss68U868Ms62MjWfqJ+ZIkSRWwCZMkSaqATZgkSVIFbMIkSZIqYBMmSZJUAZswSZKkCtiESZIkVcAmTJIkqQI2YZIkSRWwCZMkSaqATZgkSVIFbMIkSZIqYBMmSZJUAZswSZKkCtiESZIkVaBUExYRd0TEKxExExEPLbP/yoj4erH/uxHx/o59f1VsfyUiDnZv6lqv48ePs2/fPoaHh5mYmFhuSJjn9mCW9bFalvPz8wANs9werE2VkpmXXYArgFmgAfQDLwA3LxnzZ8AXitvjwNeL2zcX468E9hb3c8XlHm90dDS1eRYXF7PRaOTs7GzOz8/nyMhITk9PXzQG+JF59j6zrI8yWT766KMJvJZdyDLNc1NZmzsLMJWr1NtKS5kzYbcCM5l5OjMXgEngriVj7gK+Wtx+Avj9iIhi+2RmzmfmGWCmuD9V5MSJEwwPD9NoNOjv72d8fJyjR48uHfZezLPnmWV9lMmyWP9JsWqWPczaVFllmrAB4GzH+lyxbdkxmbkIvAnsLnmstlCr1WJoaOjt9cHBQVqt1tJh/ZhnzzPL+iiTZbG+AGbZ66xNldVX9QQAIuIQcKhYnY+I71c5ny64Fnij6kmsYBfw61/60pd+VKxfA7zn0Ucf/XHHmFs28gA1y9MszXIrlMlyP/DfNvIg5rllrM216eUsy9i33gPLNGEtYKhjfbDYttyYuYjoA66mfdq8zLFk5mPAYwARMZWZzbK/QC/q5d8hIv478L8y82Cx/lcAmfm3HWPexDyB3p6/Wa5NL8+/ZJZPAtcVt9ecZXF/5rkFrM21qcP813tsmZcjnwNuiIi9EdFP+w2Ex5aMOQbcW9z+Q+Dp4s1qx4Dx4urJvcANwIn1TlZdUSbPn2Ge24FZ1kfZf2d3F7fNsrdZmypl1TNhmfn/t3f/rFEEYQDGn7ezFKwsBLGysogWqfwAKWJhL4IiAf0GdrGwF+00hRaCWmkhYiFYaSdqFCR9wH9gKUbGYldyd3pmL3fnzGyeHywkueN4N08SJnfZzFZEXAKe0FwpuZZSWo+IVZorAh4Ct4A7EbEBfKX5gqO93z3gHbAFXEwp/ZzTuaiDjj0/AwfsWTZb9scEP2ev2LJ8fm+qq2gW3uWIiAvt06zVqv0cZjm/n4u8bLmt9vnBnoOcfz6PlcNenr+4RZgkSdJe4LZFkiRJGWRbhMUUWyGVoMP8ZyPiU0S8ao/zOeYcJyLWIuLjuMuao3GtPb/XEbGww+PZMxNbDqu5JdhzVM09bTms5pYw+57AztsWzeNgiq2QSjg6zn8WuJ571n+cw0lgAXg75vYl4DEQwCLw0p5l9rRlf1ras189bdmflrPu+fvI9UzYNFshlaDL/EVLKT2nuSJnnFPA7dR4AeyPiINj7mvPjGw5pOqWYM8RVfe05ZCqW8LMewL5Xo6cZiukEnTdVuJ0+5Tkg4g49JfbSzbJ1hn2LJst/1RrS7Bnn3ra8k+1toRdbDnlH+bPzyPgcErpGPCU7d9OVCd79oct+8We/bHnWuZahE2yFdLoFh0lVHro5AAAAPxJREFU2HH+lNKXlNL39t2bwPH/NNusdN4KpeN97ZmPLQdU3hLs2aeethxQeUuYrCeQbxE2zVZIJdhx/pHXgZeB9/9xvll4CJxpr/ZYBL6llDbH3NeeZbPlgMpbgj371NOWAypvCZP1bGS8ymAJ+EBztcTl9mOrwHL79j7gPrBBs2/WkVyz7nL+q8A6zRUgz4CjuWcemf8usAn8oHnd+hywAqy0twdwoz2/N8AJe5bZ05b9aWnPfvW0ZX9azqNnSsn/mC9JkpSDf5gvSZKUgYswSZKkDFyESZIkZeAiTJIkKQMXYZIkSRm4CJMkScrARZgkSVIGLsIkSZIy+AWQtgsOSd4nNQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x576 with 20 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SpxRdKnOJq6O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip('horizontal'),tf.keras.layers.RandomRotation(0.2),])\n",
        "# preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "# # Create the base model from the pre-trained model MobileNet V2\n",
        "# IMG_SHAPE = IMG_SIZE + (3,)\n",
        "# base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "# #image_batch, label_batch = next(iter(train_dataset))\n",
        "# feature_batch = base_model(x_train[0])\n",
        "# print(feature_batch.shape)"
      ],
      "metadata": {
        "id": "8KkfRlIOIt5n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OcjsHzd8It9I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Construct the input layer with no definite frame size.\n",
        "inp = layers.Input(shape=(None, *x_train.shape[2:]))\n",
        "\n",
        "x = layers.Reshape((-1,16,16,4))(inp)\n",
        "\n",
        "print(inp.shape)\n",
        "\n",
        "# We will construct 3 `ConvLSTM2D` layers with batch normalization,\n",
        "# followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=64,\n",
        "    kernel_size=(5, 5),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=64,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=64,\n",
        "    kernel_size=(1, 1),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.Conv3D(\n",
        "    filters=4, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
        ")(x)\n",
        "\n",
        "print(inp.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuToSoel6cH0",
        "outputId": "3bcdc1b2-2ed4-4409-b74b-c81a489ab5b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 1024)\n",
            "(None, None, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we will build the complete model and compile it.\n",
        "model = keras.models.Model(inp, x)\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),\n",
        ")\n",
        "\n",
        "\"\"\"## Model Training\n",
        "With our model and data constructed, we can now train the model.\n",
        "\"\"\"\n",
        "\n",
        "# Define some callbacks to improve training.\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "# Define modifiable training hyperparameters.\n",
        "epochs = 20\n",
        "batch_size = 5\n",
        "\n",
        "# Fit the model to the training data.\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    #validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        ")\n",
        "\n",
        "\n",
        "model.save('model_conv_lstm_v2.h5')\n",
        "\"\"\"## Frame Prediction Visualizations\n",
        "With our model now constructed and trained, we can generate\n",
        "some example frame predictions based on a new video.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D6HGoM0MLQp3",
        "outputId": "ee8732c8-6bcb-4f8f-e93a-02f6fbe45512"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0239WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 15s 394ms/step - loss: 0.0239 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0199WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 6s 363ms/step - loss: 0.0199 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0193WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 190ms/step - loss: 0.0193 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0190WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 193ms/step - loss: 0.0190 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0189WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 193ms/step - loss: 0.0189 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0188WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 189ms/step - loss: 0.0188 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0187WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 192ms/step - loss: 0.0187 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0187WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.0187 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0186WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 188ms/step - loss: 0.0186 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0186WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 189ms/step - loss: 0.0186 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0185WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 199ms/step - loss: 0.0185 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0184WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 191ms/step - loss: 0.0184 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0183WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.0183 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0183WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 196ms/step - loss: 0.0183 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0182WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 186ms/step - loss: 0.0182 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0181WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 189ms/step - loss: 0.0181 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0181WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 188ms/step - loss: 0.0181 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0180WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 188ms/step - loss: 0.0180 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0180WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 189ms/step - loss: 0.0180 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0179WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
            "15/15 [==============================] - 3s 190ms/step - loss: 0.0179 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'## Frame Prediction Visualizations\\nWith our model now constructed and trained, we can generate\\nsome example frame predictions based on a new video.'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZWhPQjX66cMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "laycUNzQ6cQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}